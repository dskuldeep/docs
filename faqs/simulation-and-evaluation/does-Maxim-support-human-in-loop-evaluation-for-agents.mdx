---
title: "Does Maxim Support Human-in-the-Loop Evaluation for Agents?"
description: "Maxim provides comprehensive support for human-in-the-loop workflows across the AI development lifecycle. You can leverage internal or external domain experts seamlessly on the platform to:"
script:
  type: "application/ld+json"
  content:
    "@context": "https://schema.org"
    "@type": "FAQPage"
    mainEntity:
      "@type": "Question"
      name: "Does Maxim Support Human-in-the-Loop Evaluation for Agents?"
      acceptedAnswer:
        "@type": "Answer"
        text: "Maxim provides comprehensive support for human-in-the-loop workflows across the AI development lifecycle. You can leverage internal or external domain experts seamlessly on the platform to balance auto-evals with human reviews, curate golden datasets, and align LLM judges."
---
- **Balance auto-evals with the last mile of human reviews**: While LLM-judges or programmatic evals provide scale, human evaluations capture nuanced quality signals that auto evals might miss.
- **Curate golden datasets**: [Human-annotated](https://www.getmaxim.ai/docs/offline-evals/via-ui/prompts/human-annotation#human-annotation) datasets are key to defining what "good" means for your specific use case, forming the foundation for effective offline evaluation.
- **Align LLM judges**:  LLM judges must be aligned with human preferences continuously to ensure they are tuned to your agent-specific outcomes.
