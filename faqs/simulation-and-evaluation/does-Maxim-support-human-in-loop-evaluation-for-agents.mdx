---
title: "Does Maxim support human-in-the-loop evaluation for agents?"
description: "Maxim provides comprehensive support for human-in-the-loop workflows across the AI development lifecycle. You can leverage internal or external domain experts seamlessly on the platform to:"
---
- **Balance auto-evals with the last mile of human reviews**: While LLM-judges or programmatic evals provide scale, human evaluations capture nuanced quality signals that auto evals might miss.
- **Curate golden datasets**: [Human-annotated](https://www.getmaxim.ai/docs/offline-evals/via-ui/prompts/human-annotation#human-annotation) datasets are key to defining what "good" means for your specific use case, forming the foundation for effective offline evaluation.
- **Align LLM judges**:  LLM judges must be aligned with human preferences continuously to ensure they are tuned to your agent-specific outcomes.
